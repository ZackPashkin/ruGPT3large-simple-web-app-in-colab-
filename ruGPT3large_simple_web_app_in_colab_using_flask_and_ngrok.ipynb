{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ruGPT3large simple web app in colab using flask and ngrok.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZackPashkin/ruGPT3large-simple-web-app-in-colab-/blob/main/ruGPT3large_simple_web_app_in_colab_using_flask_and_ngrok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhCCH15R5_Jt"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H73-Pizb6c8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b7a513-3398-40cc-d1a9-315a4711cb9e"
      },
      "source": [
        "!pip3 install transformers==3.5.0\n",
        "!pip install flask-ngrok\n",
        "!git clone https://github.com/ZackPashkin/ruGPT3large-simple-web-app-in-colab-.git\n",
        "!git clone  https://github.com/sberbank-ai/ru-gpts\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip \"/content/ngrok-stable-linux-amd64.zip\" -d /content"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 5.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 15.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.0) (54.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=ae2bd3b53f583ec5d633dee9f0b7b657dc4cee84e79d3696ab1bf9d7aa63610f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n",
            "Cloning into 'ruGPT3large-simple-web-app-in-colab-'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (14/14), done.\n",
            "Cloning into 'ru-gpts'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 595 (delta 56), reused 36 (delta 18), pack-reused 505\u001b[K\n",
            "Receiving objects: 100% (595/595), 349.36 KiB | 7.59 MiB/s, done.\n",
            "Resolving deltas: 100% (356/356), done.\n",
            "--2021-03-07 10:02:10--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.84.220.80, 52.22.246.104, 34.196.37.54, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.84.220.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  37.1MB/s    in 0.4s    \n",
            "\n",
            "2021-03-07 10:02:10 (37.1 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  /content/ngrok-stable-linux-amd64.zip\n",
            "  inflating: /content/ngrok          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA-ZZMWl6CYX"
      },
      "source": [
        "# Prepare the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csHcDJXFDdaW"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV7tt-t2FQc3"
      },
      "source": [
        "def load_tokenizer_and_model(model_name_or_path):\n",
        "  return GPT2Tokenizer.from_pretrained(model_name_or_path), GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()\n",
        "\n",
        "\n",
        "def generate(\n",
        "    model, tok, text,\n",
        "    do_sample=True, max_length=50, repetition_penalty=5.0,\n",
        "    top_k=5, top_p=0.95, temperature=1,\n",
        "    num_beams=None,\n",
        "    no_repeat_ngram_size=3\n",
        "    ):\n",
        "  input_ids = tok.encode(text, return_tensors=\"pt\").cuda()\n",
        "  out = model.generate(\n",
        "      input_ids.cuda(),\n",
        "      max_length=max_length,\n",
        "      repetition_penalty=repetition_penalty,\n",
        "      do_sample=do_sample,\n",
        "      top_k=top_k, top_p=top_p, temperature=temperature,\n",
        "      num_beams=num_beams, no_repeat_ngram_size=no_repeat_ngram_size\n",
        "      )\n",
        "  return list(map(tok.decode, out))\n",
        "  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Y_30n7C7dC"
      },
      "source": [
        "# initialize the model\n",
        "tok, ruGPT3Large = load_tokenizer_and_model(\"sberbank-ai/rugpt3large_based_on_gpt2\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MTYQNVG2MVdL",
        "outputId": "3c276044-b2e7-4a63-cdb4-f888035c0b8f"
      },
      "source": [
        "# check model inference is working\n",
        "user_input = \"рлдрдод\"\n",
        "predictions = generate(ruGPT3Large, tok, user_input, num_beams=5)\n",
        "predictions[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'рлдрдодд,\\xa0— сказал он.\\n\\n—\\xa0А что это такое?\\xa0— спросил я. Он пожал плечами и ответил: — Не знаю. Я никогда раньше не видел ничего подобного. Но оно похоже на то, что'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJckmKaRbcQP"
      },
      "source": [
        "# Run ngrok flask app"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26nCgyp5ZrEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049df842-c580-449e-996a-58fb32d29c63"
      },
      "source": [
        "# launch app\n",
        "\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template, session, request, redirect \n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "template_dir = os.path.abspath('/content/ruGPT3large-simple-web-app-in-colab-/templates')\n",
        "app = Flask(__name__, template_folder=template_dir)\n",
        "app.secret_key = 'thisIsSecret'\n",
        "run_with_ngrok(app)\n",
        "\n",
        "\n",
        "@app.route('/', methods=['POST', 'GET'])\n",
        "def home():\n",
        "  session[\"word\"] = \"\"\n",
        "  return render_template(\"index.html\")\n",
        "\n",
        "@app.route('/predict',methods = ['POST'])\n",
        "def predict(): \n",
        "  user_input = request.form[\"guess\"] + \" \"\n",
        "  print(user_input)\n",
        "  global tok \n",
        "  global ruGPT3Large\n",
        "  predictions = generate(ruGPT3Large, tok, user_input, num_beams=5)\n",
        "  # assert isinstance(user_input, str), 'Argument of wrong type!'\n",
        "  if user_input:\n",
        "    print(user_input)\n",
        "    text = \"ruGPT3 answer: \"\n",
        "    return render_template(\"predict.html\", text=text, pred=predictions[0])\n",
        "\n",
        "@app.route('/button') \n",
        "def get_ses(): \n",
        "  return redirect('/')\n",
        "  \t\n",
        "app.run()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://1526f7485ff0.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:30:19] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [07/Mar/2021 10:30:19] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "j;j;j \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:30:23] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "j;j;j \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:30:30] \"\u001b[32mGET /button HTTP/1.1\u001b[0m\" 302 -\n",
            "127.0.0.1 - - [07/Mar/2021 10:30:30] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "п \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:31:01] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "п \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:31:03] \"\u001b[32mGET /button HTTP/1.1\u001b[0m\" 302 -\n",
            "127.0.0.1 - - [07/Mar/2021 10:31:03] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Поздравление с 8 мартом от Жорика Вартанова \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:31:08] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Поздравление с 8 мартом от Жорика Вартанова \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:31:11] \"\u001b[32mGET /button HTTP/1.1\u001b[0m\" 302 -\n",
            "127.0.0.1 - - [07/Mar/2021 10:31:11] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Поздравление с 8 мартом от Жорика Вартанова \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:31:33] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Поздравление с 8 мартом от Жорика Вартанова \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [07/Mar/2021 10:31:35] \"\u001b[32mGET /button HTTP/1.1\u001b[0m\" 302 -\n",
            "127.0.0.1 - - [07/Mar/2021 10:31:35] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34wseyDMRWD6"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1UkilGMiffhC2ElZepgjdrDDBNrsm56UF \"demo\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7RAHcdmRXsN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}